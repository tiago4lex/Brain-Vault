2025-08-28 10:13

Status: #developed #seguranÃ§a 

Tags: [[CyberSecurity]] | [[Web Scraper]]

----
## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#-visÃ£o-geral)
2. [Funcionalidades](#-funcionalidades)
3. [Tecnologias Utilizadas](#-tecnologias-utilizadas)
4. [Estrutura do Projeto](#-estrutura-do-projeto)
5. [ConfiguraÃ§Ã£o do Ambiente](#-configuraÃ§Ã£o-do-ambiente)
6. [ExplicaÃ§Ã£o Detalhada do CÃ³digo](#-explicaÃ§Ã£o-detalhada-do-cÃ³digo)
7. [Estrutura do Banco de Dados](#-estrutura-do-banco-de-dados)
8. [Como Executar](#-como-executar)
9. [Exemplos de Uso](#-exemplos-de-uso)
10. [Boas PrÃ¡ticas](#-boas-prÃ¡ticas)

---
## ğŸŒŸ VisÃ£o Geral

Este Ã© um programa completo de web scraping desenvolvido em Python para extrair dados do siteÂ [Books to Scrape](https://books.toscrape.com/)Â e armazenÃ¡-los em um banco de dados MySQL. O sistema coleta informaÃ§Ãµes detalhadas de todos os livros disponÃ­veis no site e as armazena de forma estruturada para anÃ¡lise e consulta.

---
## ğŸš€ Funcionalidades

- **âœ… Scraping Completo**: Coleta dados de todas as pÃ¡ginas do catÃ¡logo
- **âœ… Armazenamento MySQL**: Armazena dados em banco relacional com relacionamentos
- **âœ… Dados Detalhados**: Extrai mais de 15 campos de informaÃ§Ã£o por livro
- **âœ… PaginaÃ§Ã£o AutomÃ¡tica**: Navega automaticamente por todas as pÃ¡ginas
- **âœ… InserÃ§Ã£o em Lote**: InserÃ§Ã£o eficiente de mÃºltiplos registros
- **âœ… Tratamento de Erros**: Robustez contra falhas de conexÃ£o
- **âœ… AnÃ¡lise de Dados**: Gera estatÃ­sticas e insights dos dados coletados
- **âœ… Respeito ao Servidor**: Implementa delays e headers apropriados

---
## ğŸ›  Tecnologias Utilizadas

| Biblioteca               | VersÃ£o | PropÃ³sito                              |
| ------------------------ | ------ | -------------------------------------- |
| `requests`               | 2.31.0 | RequisiÃ§Ãµes HTTP para acessar pÃ¡ginas  |
| `beautifulsoup4`         | 4.12.2 | Parseamento e extraÃ§Ã£o de dados HTML   |
| `mysql-connector-python` | 8.1.0  | ConexÃ£o e operaÃ§Ãµes com MySQL          |
| `python-dotenv`          | 1.0.0  | Gerenciamento de variÃ¡veis de ambiente |
| `pandas`                 | 2.0.3  | ManipulaÃ§Ã£o de dados e backup CSV      |
| `urllib`                 | 3.11.0 | ManipulaÃ§Ã£o de URLs                    |

---
## ğŸ“ Estrutura do Projeto

```txt
books_scraper_mysql.py
â”œâ”€â”€ DatabaseManager Class          # Gerenciador do banco de dados
â”‚   â”œâ”€â”€ create_connection_pool()   # Cria pool de conexÃµes
â”‚   â”œâ”€â”€ initialize_database()      # Cria tabelas no MySQL
â”‚   â”œâ”€â”€ insert_or_update_books()   # Insere dados em lote
â”‚   â””â”€â”€ get_book_stats()           # ObtÃ©m estatÃ­sticas
â”œâ”€â”€ scrape_books_toscrape()        # FunÃ§Ã£o principal de scraping
â”œâ”€â”€ extract_book_data()            # Extrai dados bÃ¡sicos do livro
â”œâ”€â”€ scrape_book_details()          # Raspa pÃ¡gina individual
â”œâ”€â”€ save_to_csv()                  # Backup em CSV
â”œâ”€â”€ analyze_data()                 # AnÃ¡lise bÃ¡sica
â””â”€â”€ main()                         # FunÃ§Ã£o principal
```

---
## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### InstalaÃ§Ã£o de DependÃªncias

```bash
pip install requests beautifulsoup4 mysql-connector-python python-dotenv pandas
```

---
## ğŸ“Š ConfiguraÃ§Ã£o do MySQL

### 1. InstalaÃ§Ã£o do MySQL

```bash
# Ubuntu/Debian
sudo apt-get install mysql-server

# Windows
# Download do MySQL Installer: https://dev.mysql.com/downloads/installer/

# macOS
brew install mysql
```

### 2. ConfiguraÃ§Ã£o do Banco de Dados

```sql
-- Conectar ao MySQL
mysql -u root -p

-- Criar banco de dados
CREATE DATABASE books_scraper CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- Criar usuÃ¡rio dedicado
CREATE USER 'scraper_user'@'localhost' IDENTIFIED BY 'sua_senha_segura';
GRANT ALL PRIVILEGES ON books_scraper.* TO 'scraper_user'@'localhost';
FLUSH PRIVILEGES;
```

### 3. Arquivo de ConfiguraÃ§Ã£o (.env)

```env
DB_HOST=localhost
DB_PORT=3306
DB_NAME=books_scraper
DB_USER=scraper_user
DB_PASSWORD=sua_senha_segura
DB_CHARSET=utf8mb4
```

---
## ğŸ” ExplicaÃ§Ã£o Detalhada do CÃ³digo

### 1.Â Classe `DatabaseManager`

```python
class DatabaseManager:
    """Gerencia todas as operaÃ§Ãµes com o banco de dados MySQL"""
    
    def __init__(self):
        self.connection_pool = self.create_connection_pool()
```

**PropÃ³sito**: Centraliza todas as operaÃ§Ãµes de banco de dados usando pool de conexÃµes para melhor performance.

### 2. Pool de ConexÃµes

```python
def create_connection_pool(self) -> pooling.MySQLConnectionPool:
    """Cria um pool de conexÃµes com configuraÃ§Ãµes do .env"""
    return pooling.MySQLConnectionPool(
        pool_name="scraper_pool",
        pool_size=5,  # 5 conexÃµes simultÃ¢neas
        host=os.getenv('DB_HOST', 'localhost'),
        database=os.getenv('DB_NAME', 'books_scraper'),
        user=os.getenv('DB_USER', 'root'),
        password=os.getenv('DB_PASSWORD', ''),
        charset=os.getenv('DB_CHARSET', 'utf8mb4')
    )
```

**Vantagens**: Reutiliza conexÃµes, melhorando performance e reduzindo overhead.

### 3. InicializaÃ§Ã£o do Banco

```python
def initialize_database(self):
    """Cria as tabelas se nÃ£o existirem"""
    create_tables_sql = [
        """
        CREATE TABLE IF NOT EXISTS categories (
            id INT AUTO_INCREMENT PRIMARY KEY,
            name VARCHAR(100) NOT NULL UNIQUE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """,
        # ... outras tabelas
    ]
```

**Funcionamento:** Executa DDL statements para criar a estrutura do banco de dados.

### 4. InserÃ§Ã£o em Lote

```python
def insert_or_update_books(self, books_data: List[Dict]):
    """Insere mÃºltiplos livros de uma vez usando executemany"""
    books_to_insert = []
    for book in books_data:
        # Preprocessa dados
        price = self._convert_price(book.get('PreÃ§o', ''))
        # ... outros campos
        
        books_to_insert.append((
            book.get('UPC'), book.get('TÃ­tulo'), price, # ... 
        ))
    
    cursor.executemany(insert_sql, books_to_insert)
```

**Performance**: InserÃ§Ã£o em lote Ã© muito mais rÃ¡pida que inserÃ§Ãµes individuais.

### 5. FunÃ§Ã£o Principal: `scrap_books_toscrap()`

```python
def scrape_books_toscrape():
    """Coordena o scraping de todas as pÃ¡ginas"""
    base_url = "https://books.toscrape.com/"
    all_books = []
    page = 1
    
    while True:  # Loop atÃ© nÃ£o encontrar mais pÃ¡ginas
        url = f"{base_url}catalogue/page-{page}.html" if page > 1 else base_url
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Encontrar livros na pÃ¡gina
        books = soup.find_all('article', class_='product_pod')
```

**Como funciona a paginaÃ§Ã£o:**

- O site usa URLs sequenciais: `/catalogue/page-2.html`, `/catalogue/page-3.html`, etc.
- A primeira pÃ¡gina Ã© a URL base, as demais seguem o padrÃ£o acima.
- O loop continua atÃ© encontrar a mensagem *"No results found."* ou atÃ© que nÃ£o haja mais livros.

### 6. Estrutura HTML do Site e Seletores

**HTML TÃ­pico de um Livro:**

```html
<article class="product_pod">
    <div class="image_container">
        <a href="catalogue/a-light-in-the-attic_1000/index.html">
            <img src="media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg" 
                 alt="A Light in the Attic" class="thumbnail">
        </a>
    </div>
    <p class="star-rating Three">  <!-- 3 estrelas -->
        <i class="icon-star"></i>
        <i class="icon-star"></i>
        <i class="icon-star"></i>
        <i class="icon-star"></i>
        <i class="icon-star"></i>
    </p>
    <h3><a href="catalogue/a-light-in-the-attic_1000/index.html" 
           title="A Light in the Attic">A Light in the Attic</a></h3>
    <div class="product_price">
        <p class="price_color">Â£51.77</p>
        <p class="instock availability">
            <i class="icon-ok"></i> In stock
        </p>
        <form>
            <button type="submit" class="btn btn-primary btn-block">Add to basket</button>
        </form>
    </div>
</article>
```

### 7. Extraindo Dados BÃ¡sicos: `extract_book_data()`

```python
def extract_book_data(book, base_url):
    """Extrai dados de um elemento livro"""
    title = book.h3.a['title']  # TÃ­tulo do atributo title
    price = book.find('p', class_='price_color').get_text(strip=True)
    availability = book.find('p', class_='instock availability').get_text(strip=True)
    
    # Sistema de avaliaÃ§Ã£o por estrelas
    rating_class = book.find('p', class_='star-rating')['class'][1]
    rating_map = {'One': '1 estrela', 'Two': '2 estrelas', ...}
    rating = rating_map.get(rating_class, 'NÃ£o avaliado')
```

**Seletores CSS utilizados:**

- `.product_pod`Â â†’ Container do livro
- `.price_color`Â â†’ PreÃ§o
- `.star-rating`Â â†’ AvaliaÃ§Ã£o
- `.instock.availability`Â â†’ Disponibilidade

### 8.Â Sistema de AvaliaÃ§Ã£o por Estrelas

```python
# As avaliaÃ§Ãµes sÃ£o representadas por classes CSS:
rating_class = book.find('p', class_='star-rating')['class'][1]
rating_map = {
    'One': '1 estrela',      # Classe: "star-rating One"
    'Two': '2 estrelas',     # Classe: "star-rating Two"
    'Three': '3 estrelas',   # Classe: "star-rating Three"
    'Four': '4 estrelas',    # Classe: "star-rating Four"
    'Five': '5 estrelas'     # Classe: "star-rating Five"
}
rating = rating_map.get(rating_class, 'NÃ£o avaliado')
```

### 9. Scraping de PÃ¡ginas Individuais: `scrap_book_details()`

Para obter informaÃ§Ãµes detalhadas, o programa visita a pÃ¡gina individual de cada livro:

```python
def scrape_book_details(book_url):
    """Raspa detalhes da pÃ¡gina individual do livro"""
    response = requests.get(book_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Meta description
    description = soup.find('meta', attrs={'name': 'description'})['content']
    
    # Tabela de metadados
    product_info = {}
    table = soup.find('table', class_='table table-striped')
    for row in table.find_all('tr'):
        header = row.find('th').get_text(strip=True)  # Ex: "UPC"
        value = row.find('td').get_text(strip=True)   # Ex: "a897fe39b1053632"
        product_info[header] = value
```

**Estrutura da Tabela de Metadados**

```html
<table class="table table-striped">
    <tr>
        <th>UPC</th>
        <td>a897fe39b1053632</td>
    </tr>
    <tr>
        <th>Product Type</th>
        <td>Books</td>
    </tr>
    <tr>
        <th>Price (excl. tax)</th>
        <td>Â£51.77</td>
    </tr>
    <!-- ... mais linhas ... -->
</table>
```

### 10. ConversÃ£o de PreÃ§os

```python
def _convert_price(self, price_str: str) -> Optional[float]:
    """Converte 'Â£51.77' para 51.77"""
    if not price_str or price_str == 'N/A':
        return None
    try:
        return float(price_str.replace('Â£', '').strip())
    except ValueError:
        return None
```

### 11. NavegaÃ§Ã£o no Breadcrumb para Categoria

```python
# O breadcrumb mostra a hierarquia de navegaÃ§Ã£o
# Exemplo: Home > Books > Poetry
category = ""
breadcrumb = soup.find('ul', class_='breadcrumb')
if breadcrumb:
	breadcrumb_items = breadcrumb.find_all('li')
	if len(breadcrumb_items) >= 3:
		category = breadcrumb_items[2].get_text(strip=True)
```

**Estrutura do Breadcrumb:**

```html
<ul class="breadcrumb">
    <li><a href="index.html">Home</a></li>
    <li><a href="catalogue/category/books_1/index.html">Books</a></li>
    <li><a href="catalogue/category/books/poetry_23/index.html">Poetry</a></li>
</ul>
```

### 12. Tratamento de URLs Relativas

```python
# Converte URLs relativas para absolutas
from urllib.parse import urljoin

relative_link = book.h3.a['href']  # Ex: "catalogue/book-name/index.html"
book_link = urljoin(base_url, relative_link)  # Ex: "https://books.toscrape.com/catalogue/book-name/index.html"

image_url = soup.find('img')['src']  # Ex: "../../media/cache/image.jpg"
full_image_url = urljoin(book_url, image_url)  # URL absoluta completa
```

### 13. Gerenciamento de Erros e Robustez

```python
try:
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Levanta exceÃ§Ã£o para status 4xx/5xx
    
except requests.RequestException as e:
    print(f"Erro na requisiÃ§Ã£o: {e}")
    # Continua para a prÃ³xima pÃ¡gina em vez de quebrar o programa

# Fallbacks para seletores que podem falhar
title = book.h3.a['title'] if book.h3 and book.h3.a else 'TÃ­tulo nÃ£o disponÃ­vel'
```

### 14. ExportaÃ§Ã£o para CSV com Pandas

```python
def save_to_csv(books_data, filename='books_toscrape.csv'):
    """
    Converte lista de dicionÃ¡rios para DataFrame e exporta para CSV
    """
    df = pd.DataFrame(books_data)  # Cria DataFrame from list of dicts
    
    # Cria diretÃ³rio se nÃ£o existir
    os.makedirs('dados', exist_ok=True)
    filepath = os.path.join('dados', filename)
    
    # Exporta com encoding UTF-8 para suportar caracteres especiais
    df.to_csv(filepath, index=False, encoding='utf-8')
```

### 15. AnÃ¡lise e EstatÃ­sticas dos Dados

```python
Â # Obter estatÃ­sticas do banco
stats = db_manager.get_book_stats()
print("\n=== ğŸ“Š ESTATÃSTICAS DO BANCO ===")
print(f"ğŸ“š Total de livros: {stats.get('total_books', 0)}")
print(f"ğŸ’° PreÃ§o mÃ©dio: Â£{stats.get('avg_price', 0):.2f}")
print(f"ğŸ·ï¸ Categorias Ãºnicas: {stats.get('unique_categories', 0)}")

if stats.get('top_categories'):
	print("\nğŸ† Top 5 categorias:")
	for category in stats['top_categories']:
		print(f" Â {category['name']}: {category['book_count']}
```

---
## ğŸ—ƒï¸ Estrutura do Banco de Dados

### Diagrama do Schema

```text
books_scraper
â”œâ”€â”€ categories
â”‚   â”œâ”€â”€ id (PK)
â”‚   â”œâ”€â”€ name (UNIQUE)
â”‚   â””â”€â”€ created_at
â”‚
â””â”€â”€ books
    â”œâ”€â”€ id (PK)
    â”œâ”€â”€ upc (UNIQUE)
    â”œâ”€â”€ title
    â”œâ”€â”€ price (DECIMAL)
    â”œâ”€â”€ category_id (FK â†’ categories.id)
    â”œâ”€â”€ rating
    â”œâ”€â”€ description (TEXT)
    â””â”€â”€ scraped_at
```

### Script de CriaÃ§Ã£o das Tabelas

```sql
USE books_scraper;

CREATE TABLE categories (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100) NOT NULL UNIQUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE books (
    id INT AUTO_INCREMENT PRIMARY KEY,
    upc VARCHAR(50) UNIQUE,
    title VARCHAR(255) NOT NULL,
    price DECIMAL(10, 2),
    price_excl_tax DECIMAL(10, 2),
    price_incl_tax DECIMAL(10, 2),
    tax DECIMAL(10, 2),
    availability VARCHAR(100),
    rating VARCHAR(20),
    category_id INT,
    product_type VARCHAR(100),
    description TEXT,
    number_of_reviews INT DEFAULT 0,
    image_url VARCHAR(500),
    book_url VARCHAR(500),
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (category_id) REFERENCES categories(id) ON DELETE SET NULL,
    INDEX idx_books_title (title),
    INDEX idx_books_price (price),
    INDEX idx_books_category (category_id),
    INDEX idx_books_upc (upc)
);
```

---
## ğŸš€ Como Executar

### 1. Arquivo de Requisitos (`requirements.txt`)

```txt
requests==2.31.0
beautifulsoup4==4.12.2
pandas==2.0.3
mysql-connector-python==8.1.0
python-dotenv==1.0.0
urllib==3.11.0
```

### 2. PreparaÃ§Ã£o do Ambiente

```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar arquivo .env
echo "DB_HOST=localhost" > .env
echo "DB_PORT=3306" >> .env
echo "DB_NAME=books_scraper" >> .env
echo "DB_USER=scraper_user" >> .env
echo "DB_PASSWORD=sua_senha_segura" >> .env
```

### 3. ExecuÃ§Ã£o

```bash
python books_scraper_mysql.py
```

### 4. VerificaÃ§Ã£o dos Dados

```bash
# Acessar MySQL
mysql -u scraper_user -p books_scraper

-- Verificar dados
SELECT COUNT(*) as total_livros FROM books;
SELECT title, price, rating FROM books LIMIT 10;
```

### SaÃ­da Esperada

```text
=== WEB SCRAPING - BOOKS TO SCRAPE COM MYSQL ===
âœ… Banco de dados inicializado com sucesso!
ğŸ”„ Iniciando scraping...
ğŸ“š Iniciando scraping do Books to Scrape...
ğŸ“„ Raspando pÃ¡gina 1: https://books.toscrape.com/
âœ… Encontrados 20 livros na pÃ¡gina 1
ğŸ“„ Raspando pÃ¡gina 2: https://books.toscrape.com/catalogue/page-2.html
âœ… Encontrados 20 livros na pÃ¡gina 2
ğŸ“„ Raspando pÃ¡gina 3: https://books.toscrape.com/catalogue/page-3.html
...
âœ… NÃ£o hÃ¡ mais pÃ¡ginas. Finalizando...
ğŸ’¾ Salvando dados no MySQL...
âœ… Inseridos/Atualizados 980 livros no banco de dados
âœ… Backup salvo em: dados\books_toscrape.csv
ğŸ“Š Total de livros no backup: 1000

=== ğŸ“Š ESTATÃSTICAS DO BANCO ===
ğŸ“š Total de livros: 1001
ğŸ’° PreÃ§o mÃ©dio: Â£35.05
ğŸ·ï¸ Categorias Ãºnicas: 51

ğŸ† Top 5 categorias:
  Default: 152 livros
  Nonfiction: 110 livros
  Sequential Art: 75 livros
  Add a comment: 67 livros
  Fiction: 65 livros
```

### Banco de Dados

![[Pasted image 20250828145844.png]]

### Programa em AÃ§Ã£o

![[Pasted image 20250828150503.png]]

### Resultados

![[Pasted image 20250828151742.png]]

---
## ğŸ“Š Exemplos de Uso

### Consultas SQL Ãšteis

```sql
-- Livros por categoria
SELECT c.name as categoria, COUNT(b.id) as total
FROM books b
JOIN categories c ON b.category_id = c.id
GROUP BY c.name
ORDER BY total DESC;

-- PreÃ§o mÃ©dio por categoria
SELECT c.name, AVG(b.price) as preco_medio
FROM books b
JOIN categories c ON b.category_id = c.id
GROUP BY c.name
ORDER BY preco_medio DESC;

-- Top 10 livros mais caros
SELECT title, price, rating, availability
FROM books
ORDER BY price DESC
LIMIT 10;
```

### AnÃ¡lise com Python

```python
# ApÃ³s o scraping, analyze os dados
stats = db_manager.get_book_stats()
print(f"Total de livros: {stats['total_books']}")
print(f"PreÃ§o mÃ©dio: Â£{stats['avg_price']:.2f}")
print(f"Categoria com mais livros: {stats['top_categories'][0]['name']}")
```

---
## âš¡ Boas PrÃ¡ticas de Web Scraping Implementadas

### 1. GestÃ£o de ConexÃµes

```python
# Uso de connection pool
with connection_pool.get_connection() as conn:
    with conn.cursor() as cursor:
        cursor.execute(...)
```

### 2. Tratamento de Erros

```python
try:
    # OperaÃ§Ã£o de banco
except mysql.connector.Error as e:
    print(f"Erro MySQL: {e}")
    if connection:
        connection.rollback()
```

### 3. PrevenÃ§Ã£o de SQL Injection

```python
# Uso de parÃ¢metros seguros
cursor.execute("INSERT INTO books (title, price) VALUES (%s, %s)", 
               (title, price))
```

### 4. Respeito ao Servidor

```python
# Delay entre requisiÃ§Ãµes
time.sleep(1)  # 1 segundo entre pÃ¡ginas

# Headers realistas
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8'
}
```

### 5. Backup e ResilÃªncia

```python
# Backup em CSV alÃ©m do MySQL
def save_to_csv(books_data):
    df = pd.DataFrame(books_data)
    df.to_csv('backup_books.csv', index=False, encoding='utf-8')
```

---
## ğŸ› PossÃ­veis Problemas e SoluÃ§Ãµes

### 1. Erro de ConexÃ£o MySQL

```bash
# Verificar se MySQL estÃ¡ rodando
sudo service mysql start

# Verificar credenciais no .env
```

### 2. Erro de PermissÃµes

```sql
-- Conceder permissÃµes
GRANT ALL PRIVILEGES ON books_scraper.* TO 'scraper_user'@'localhost';
FLUSH PRIVILEGES;
```

### 3. Timeout de ConexÃ£o

```python
# Aumentar timeout
response = requests.get(url, timeout=30)
```

---
## ğŸ“ˆ PrÃ³ximos Passos e Melhorias PossÃ­veis

1. **ğŸ”„ Scraping Incremental**: Atualizar apenas dados modificados
2. **ğŸ“Š Dashboard Web**: Interface visual para os dados
3. **ğŸ” Busca AvanÃ§ada**: Funcionalidade de search no banco
4. **ğŸ“± API REST**: Endpoints para acesso programÃ¡tico
5. **ğŸ¤– AutomaÃ§Ã£o**: Agendamento de scraping periÃ³dico

---
## ğŸ“ LicenÃ§a e ConsideraÃ§Ãµes Ã‰ticas

Este projeto Ã© para fins educacionais. O site Books to Scrape foi criado especificamente para praticar web scraping. Ao fazer scraping de sites reais:

- âœ… Verifique oÂ `robots.txt`
- âœ… Respeite os termos de serviÃ§o
- âœ… Use delays apropriados
- âœ… NÃ£o sobrecarregue os servidores
- âœ… Use os dados coletados de forma Ã©tica