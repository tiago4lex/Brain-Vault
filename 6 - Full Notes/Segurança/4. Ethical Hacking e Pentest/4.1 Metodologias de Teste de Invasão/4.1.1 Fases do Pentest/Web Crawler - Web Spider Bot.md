2025-08-13 22:40

Status: #developed #segurança 

Tags: [[CyberSecurity]] | [[Web Crawler]]

----
# Introdução

## O que é um Web Crawler

Um **Web Crawler** (também conhecido como *spider*, *bot* ou *web spider*) é um programa automatizado que **navega pela internet** de forma sistemática, acessando páginas web, lendo seu conteúdo e extraindo informações.

Na prática, um crawler funciona como um "robô" que **segue linkss*, coleta dados e armazena para indexação, análise outro propósito.

## Finalidades comuns

- **Indexação de sites** (ex.: Googlebot, Bingbot).
- **Coleta de dados** (*web scraping*).
- **Monitoramento de mudanças** em páginas.
- **Análise de vulnerabilidades** (no contexto de cibersegurança).

---
# Como funciona um Web Crawler?

O funcionamento pode ser dividido em etapas:

1. **Ponto de partida *(Seed URL)***
	- O crawler começa com  uma lista inicial de URLs a visitar.

2. **Download do conteúdo**
	- O programa envia requisições HTTP/HTTPS para o servidor, recuperando o HTML da página.

3. **Extração de links**
	- Analisa o HTML e identifica todos os links internos  e externos.

4. **Agendamento e fila de URLs**
	- Os novos links encontrados são adicionados a uma fila para visitas futuras.
	- Evita visitas repetidas e prioriza páginas importantes.

5. **Respeito a regras (`robots.txt`)**
	- Antes de acessar um site, verifica o arquivo `robots.txt` para saber quais páginas o robô pode ou não visitar.

6. **Extração de dados**
	- Pode extrair textos, imagens, metadados ou informações específicas (como e-mails, preços, palavras-chave).

7. **Armazenamento**
	- Os dados coletados são salvos em bancos de dados, arquivos CSV, JSON ou sistemas de indexação.

---
# Arquitetura Básica

Um Web Crawler geralmente contém

- **Módulo de Coleta** - responsável por baixar páginas.
- **Parser (Analisador)** - para extrair links e informações.
- **Gerenciador de URLs** - para organizar as páginas a visitar.
- **Armazenamento** - onde os dados coletados ficam guardados.
- **Controle de Taxa *(Rate Limiting)*** - evita sobrecarregar servidores.

---
# Aplicações na Cibersegurança

Na segurança da informação, crawlers podem ser usados para:

- **Mapeamento de superfícies de ataque** (descoberta de diretórios e subdomínios).
- **Coleta de informações para OSINT** *(Open Source Intelligence)*.
- **Detecção de alterações em páginas** (possíveis defacements).
- **Coleta de credenciais expostas** ou dados sensíveis publicados inadvertidamente.
- **Enumeração de páginas ocultas** não ligadas no site principal.

**Exemplo prático na cibersegurança:**

Um analista pode usar um crawler para vasculhar um site à procura de arquivos de backup (`.zip`, `.sql`), pastas de configurações ou endpoints de API expostos.

---
# Ferramentas populares de Web Crawling

## 1. Ferramentas para Coleta Geral

- **Scrapy** (Python) - Framework poderoso para crawling e scraping.
- **BeautifulSoup** (Python) - Ótimo para extrair dados de HTML.
- **Selenium** - Automatiza navegadores, útil para sites dinâmicos.
- **Puppeteer** (Node.js) - Automação de páginas web com Chrome Headless.

## 2. Ferramentas para Cibersegurança

- **OWASP Amass** - Enumeração de subdomínios e mapeamento de redes.
- **dirsearch** - Busca por diretórios ocultos.
- **gobuster** - Enumeração rápida de diretórios e arquivos.
- **hakrawler** - Crawler rápido em Go, focado em segurança.
- **Burp Suite Spider** - Integrado no Burp Suite, usado para mapear aplicações web.

---
# Como usar um Web Crawler

Exemplo simples com Python + Requestes + Beautiful Soup

```python
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

visitadas = set()
fila = ["https://exemplo.com"]

while fila:
    url = fila.pop(0)
    if url in visitadas:
        continue

    try:
        resposta = requests.get(url, timeout=5)
        if resposta.status_code != 200:
            continue

        print(f"Visitando: {url}")
        visitadas.add(url)

        soup = BeautifulSoup(resposta.text, "html.parser")
        for link in soup.find_all("a", href=True):
            novo_link = urljoin(url, link["href"])
            if novo_link not in visitadas:
                fila.append(novo_link)

    except requests.RequestException:
        continue
```

Esse script:

- Começa com uma URL inicial.
- Baixa o HTML
- Extrai links e os adiciona na fila.
- Evita revisitar a mesma página.

---
# Exemplos de Uso no Mundo Real

- **Googlebot** - Varre bilhões de páginas para indexação no Google Search.
- **Shodan** - Embora seja mais um scanner que um crawler tradicional, também coleta informações de dispositivos expostos na internet.
- **Wayback Machine (Internet Archive)** - Faz crawling e arquiva páginas para consulta histórica.
- **Crawlers de monitoramento** - Empresas usam para verificar uptime e alterações de conteúdo.

---
# Boas Práticas e Considerações Éticas

 - **Respeitar o `robots.txt`** do site.
- **Não sobrecarregar servidores** (usar _rate limiting_).
- **Evitar violações legais** – Coletar dados de forma não autorizada pode infringir leis como LGPD (Brasil) ou GDPR (Europa).
- **Usar apenas em ambientes autorizados** – No contexto de cibersegurança, sempre com permissão explícita.

---
# Dicas para Aprender Mais

- Estudar **Scrapy** para projetos grandes.
- Testar **hakrawler** e **Amass** para fins de segurança.
- Praticar em **sites de teste** como:
    - http://books.toscrape.com/ (para scraping)
    - https://hackthissite.org/ (para segurança)

- Criar seu próprio crawler e testar em ambientes locais.

---
# Conclusão

O **Web Crawler** é uma ferramenta essencial tanto para **indexação e pesquisa de informações** quanto para **cibersegurança** e **OSINT**.  
Combinando-o com análise e ferramentas específicas, é possível descobrir vulnerabilidades, monitorar alterações e automatizar a coleta de dados em larga escala.  
Entender seu funcionamento e limitações é fundamental para aplicá-lo de forma **eficaz, ética e legal**.